plot(res.1)
rm(list=ls())
x<-seq(from = -5, to = 5, by = 1)
x
## [1] 3
x[[length(x)]]
x<-seq(from = -5, to = 5, by = 1)
x
a<-2 #intercept
b<-1 #slope
y<-a+b*x
plot(x,y)
segments(0,-10,0,10, lty=3) #add axes
segments(-10,0,10,0,lty=3)
plot(x,y)
segments(0,-10,0,10, lty=3) #add axes
segments(-10,0,10,0,lty=3)
?abline
plot(x,y, col="white")
segments(0,-10,0,10, lty=3)
segments(-10,0,10,0,lty=3)
abline(a = 2, b=1)
plot(x,y, col="green")
segments(0,-10,0,10, lty=3)
segments(-10,0,10,0,lty=3)
plot(x,y, col="black")
segments(0,-10,0,10, lty=3)
segments(-10,0,10,0,lty=3)
abline(a = 2, b=1)
points(4,0, col="red", pch=19)
points(-2,6, col="green", pch=9)
points(x,y, pch=c(1,2,3,4,5,6,7,8,9,10,11))
y<-x^2
plot(x,y)
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3)
segments(0,-40,0,30, lty=3)
segments(-30,0,30,0,lty=3)
segments(-30,20,30,0,lty=3)
segments(-30,0,30,0,lty=3)
segments(-30,0,30,0,lty=3)
y<-x^2
plot(x,y)
segments(-30,0,30,0,lty=3)
segments(-30,0,30,0,lty=3)
segments(-30,0,30,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(-30,0,30,0,lty=3)
segments(0,-30,0,30, lty=3)
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(0,-30,0,30, lty=3)
segments(0,-5,0,30, lty=3)
segments(0,-5,0,5, lty=3)
segments(0,-5,0,3, lty=3)
segments(0,-3,0,3, lty=3)
y<-x^2
plot(x,y)
segments(-30,0,30,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(0,-3,0,3, lty=3)
y<-x^2
plot(x,y)
segments(-30,0,30,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(0,-3,0,3, lty=3)
segments(0,-30,0,30, lty=3)
y<-x^2
plot(x,y)
segments(-3,0,3,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3) #starts at point -30, 0 and ends at point 30, 0
segments(0,-30,0,30, lty=3)
#give linear function intercept of a-2
x<-seq(from = -5, to = 5, by = 0.1)
a<- -2
y<-a+x^2
plot(x,y)
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3)
plot(x,y)
a<- -2
b<-3
y<-a+b*x^2
points(x,y, pch=19, col="red")
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3)
y<-a+b*x^2 #gave the curve a higher slope
points(x,y, pch=19, col="red")
segments(0,-30,0,30, lty=3)
segments(-30,0,30,0,lty=3)
plot(x,y)
a<- -2
b1<- 10
b2<-3
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-100,0,100, lty=3)
segments(-100,0,100,0,lty=3)
plot(x,y)
a<- 1
b1<- 2
b2<-0.15
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-100,0,100, lty=3)
segments(-100,0,100,0,lty=3)
plot(x,y)
a<- 1
b1<- 2
b2<-0.15
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-100,0,100, lty=3)
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
plot(x,y)
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x+b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
plot(x,y)
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x-b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
plot(x,y)
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x-b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
plot(x, y, type="n", xlim=c(-100, 100), ylim=c(-1000, 1000), main="Plot with Adjusted Scales")
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x-b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
segments(-1000,0,1000,0,lty=3)
max_y_index <- which.max(y)  # Find the index of the maximum y value
highest_x <- x[max_y_index]  # Get the corresponding x value
highest_y <- y[max_y_index]  # Get the highest y value# Find the index of the maximum y value
points(highest_x, highest_y, col="red", pch=19)
x<-seq(from = -100, to = 100, by = 1)
plot(x, y, type="n", xlim=c(-100, 100), ylim=c(-1000, 1000), main="Plot with Adjusted Scales")
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x-b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
max_y_index <- which.max(y)  # Find the index of the maximum y value
highest_x <- x[max_y_index]  # Get the corresponding x value
highest_y <- y[max_y_index]  # Get the highest y value# Find the index of the maximum y value
points(highest_x, highest_y, col="red", pch=19)
text(highest_x, highest_y, labels=paste("Max y =", round(highest_y, 2)), pos=3)
#find highest y point in equation
x<-seq(from = -100, to = 100, by = 1)
plot(x, y, type="n", xlim=c(-100, 100), ylim=c(-1000, 1000), main="Plot with Adjusted Scales")
a<- -1
b1<- 2
b2<-0.15
y<-a+b1*x-b2*x^2
points(x,y, pch=19, col="green")
segments(0,-1000,0,1000, lty=3)
segments(-1000,0,1000,0,lty=3)
max_y_index <- which.max(y)  # Find the index of the maximum y value
highest_x <- x[max_y_index]  # Get the corresponding x value
highest_y <- y[max_y_index]  # Get the highest y value# Find the index of the maximum y value
points(highest_x, highest_y, col="red", pch=19)
text(highest_x, highest_y, labels=paste("Max y =", round(highest_y, 2)), pos=3) # pos=3 places the text above the point for better visibility
library(dplyr)
data_groups <- data %>% group_by(ID) %>% group_split() # Split the data by ID
data_groups <- data %>% group_by(ID) %>% group_split() # Split the data by ID
#Local paralellisation
#run tasks in parallel for greater speed/efficiency
n <- 10000 #number of observations
data <- data.frane(
ID = sample(1:10, n, replace = TRUE), # ID column to define 10 groups
Y = rnorm(n), #creates a vector of 10000 random values from normal standard dist (mean=0,sd=1)
x = rnorm(n)
)
data <- data.frame(
ID = sample(1:10, n, replace = TRUE), # ID column to define 10 groups
Y = rnorm(n), #creates a vector of 10000 random values from normal standard dist (mean=0,sd=1)
x = rnorm(n)
)
library(dplyr)
data_groups <- data %>% group_by(ID) %>% group_split() # Split the data by ID
View(data_groups)
# Define a function to fit a linear model for each group
fit_model <- function(group_data) {
model <- lm(y ~ X, data = group_data)
coef_df <- as.data.frame(t(coef(model)))
coef_df$ID <- unique(group_data$ID)  # Add ID for reference
return(coef_df)
}
library(dplyr)
data_groups <- data %>% group_by(ID) %>% group_split() # Split the data by ID
# Define a function to fit a linear model for each group
fit_model <- function(group_data) {
# Fit a linear model (lm) to the data in group_data, predicting y using X
model <- lm(y ~ X, data = group_data)
# Extract the coefficients from the model and transpose them to create a one-row data frame
coef_df <- as.data.frame(t(coef(model)))
# Add a column to the coefficients data frame with the unique ID from the group_data for reference
coef_df$ID <- unique(group_data$ID)
# Return the data frame containing the model coefficients and the ID
return(coef_df)
}
library(parallel)
num_cores <- detectCores() - 1 # Use all cores but one
results <- mclapply(data_groups, fit_model, mc.cores = num_cores) # Fit the models
final_results <- bind_rows(results) # Bind model outputs from list to table
print(final_results)
library(dplyr)
data_groups <- data %>% group_by(ID) %>% group_split() # Split the data by ID
# Define a function to fit a linear model for each group
fit_model <- function(group_data) {
# Fit a linear model (lm) to the data in group_data, predicting y using X
model <- lm(y ~ X, data = data_groups)
# Extract the coefficients from the model and transpose them to create a one-row data frame
coef_df <- as.data.frame(t(coef(model)))
# Add a column to the coefficients data frame with the unique ID from the group_data for reference
coef_df$ID <- unique(group_data$ID)
# Return the data frame containing the model coefficients and the ID
return(coef_df)
}
library(parallel)
num_cores <- detectCores() - 1 # Use all cores but one
results <- mclapply(data_groups, fit_model, mc.cores = num_cores) # Fit the models
final_results <- bind_rows(results) # Bind model outputs from list to table
print(final_results)
library(parallel)
num_cores <- detectCores() - 1 # Use all cores but one
results <- mclapply(data_groups, fit_model, mc.cores = num_cores) # Fit the models
final_results <- bind_rows(results) # Bind model outputs from list to table
print(final_results)
# Define a function to fit a linear model for each group
fit_model <- function(group_data) {
# Fit a linear model (lm) to the data in group_data, predicting Y using x
model <- lm(Y ~ x, data = group_data)
# Extract the coefficients from the model and transpose them to create a one-row data frame
coef_df <- as.data.frame(t(coef(model)))
# Add a column to the coefficients data frame with the unique ID from the group_data for reference
coef_df$ID <- unique(group_data$ID)
# Return the data frame containing the model coefficients and the ID
return(coef_df)
}
# Now, run the parallel code to fit models for each group
library(parallel)
num_cores <- detectCores() - 1  # Use all cores but one
# Use mclapply to run the fit_model function on each subset of the data
results <- mclapply(data_groups, fit_model, mc.cores = num_cores)
# Bind all the results into one data frame
final_results <- bind_rows(results)
print(final_results)
?nr
?nr
?sample
sim_genetic_drift<-function(N=10, t=5, p0=0.5)
{
# THE BIG LIST OF MATRICES (TO BE FILLED IN LATER)
population <-list()
length(population)<-t+1
# GIVE NAMES TO THE ELEMENTS (Generation 0, generation 2 etc)
for (i in 1:(t+1))
{
names(population)[i]<-paste(c('generation', i-1), collapse='') #i-1 ensures correct stating position and collapse combines itno single string
}
# ALSO KEEP TRACK ON THE ALLELE FREQ OVER TIME, AS A VECTOR (Fill with NAS TO BE OVERWRITTEN)
allele.freq<-rep(NA, t+1)
# INITIALISE
# NUMBER OF "0" ALLELE AT THE START, GOVERNED BY p0
k<-round(2*N*p0)
population[[1]]<-matrix(sample(c(rep(0, k), rep(1, 2*N-k))), nr=2) # creates 2 row matrix that uses sample() to randomly sample from a vector that contains k copies of allele "0" and 2 * N - k copies of allele "1" to the individuals.
# THE INITIAL ALLELE FREQ
allele.freq[1]<-sum(population[[1]]==0)/(2*N) #n times 2 because total number of alleles is twice that of pop (diploid)
# PROPAGATION (a for loop (length of generations is ran))
for (i in 1:t)
{
# THE GAMETIC FREQ IS JUST THE ALLELE FREQ FROM THE PARENTAL GEN
population[[i+1]]<-matrix(sample(0:1, size=2*N, prob=c(allele.freq[i], 1-allele.freq[i]), replace=T), nr=2) #randomly samples for next gen, 0:1 are sampled for total size of alleles (N*2), popubabilities are specified for 0 and 1. sampling is replaced to ensure odds stay the same (not removed from pool when sampled) and stored in matrix with 2 rows
# UPDATE NEW FREQ
allele.freq[i+1]<-sum(population[[i+1]]==0)/(2*N)
}
# RETURN A BIG LIST, EXIT
return(list(population=population, allele.freq=allele.freq))
}
# RETURN A BIG LIST, EXIT
return(list(population=population, allele.freq=allele.freq))
sim_genetic_drift<-function(N=10, t=5, p0=0.5)
{
# THE BIG LIST OF MATRICES (TO BE FILLED IN LATER)
population <-list()
length(population)<-t+1
# GIVE NAMES TO THE ELEMENTS (Generation 0, generation 2 etc)
for (i in 1:(t+1))
{
names(population)[i]<-paste(c('generation', i-1), collapse='') #i-1 ensures correct stating position and collapse combines itno single string
}
# ALSO KEEP TRACK ON THE ALLELE FREQ OVER TIME, AS A VECTOR (Fill with NAS TO BE OVERWRITTEN)
allele.freq<-rep(NA, t+1)
# INITIALISE
# NUMBER OF "0" ALLELE AT THE START, GOVERNED BY p0
k<-round(2*N*p0)
population[[1]]<-matrix(sample(c(rep(0, k), rep(1, 2*N-k))), nr=2) # creates 2 row matrix that uses sample() to randomly sample from a vector that contains k copies of allele "0" and 2 * N - k copies of allele "1" to the individuals.
# THE INITIAL ALLELE FREQ
allele.freq[1]<-sum(population[[1]]==0)/(2*N) #n times 2 because total number of alleles is twice that of pop (diploid)
# PROPAGATION (a for loop (length of generations is ran))
for (i in 1:t)
{
# THE GAMETIC FREQ IS JUST THE ALLELE FREQ FROM THE PARENTAL GEN
population[[i+1]]<-matrix(sample(0:1, size=2*N, prob=c(allele.freq[i], 1-allele.freq[i]), replace=T), nr=2) #randomly samples for next gen, 0:1 are sampled for total size of alleles (N*2), popubabilities are specified for 0 and 1. sampling is replaced to ensure odds stay the same (not removed from pool when sampled) and stored in matrix with 2 rows
# UPDATE NEW FREQ
allele.freq[i+1]<-sum(population[[i+1]]==0)/(2*N)
}
# RETURN A BIG LIST, EXIT
return(list(population=population, allele.freq=allele.freq))
}
# TEST RUN
sim_genetic_drift()
sim_genetic_drift2<-function(N=10, t=5, p0=0.5)
{
# A VECTOR FOR ALLELE FREQ
allele.freq<-rep(NA, t+1)
# INITIALISE
allele.freq[1]<-p0
# PROPAGATION
# DIVIDE COUNT BY 2*N TO GET FREQ
for (i in 1:t)
{
allele.freq[i+1]<-rbinom(1, size=2*N, prob=allele.freq[i])/(2*N)
}
# RETURN AND EXIT
return(allele.freq)
}
sim_genetic_drift2()
# SMALL N=50 SCENARIO, WITH 20 REPLICATEAS
af_N50<-matrix(nr=20, nc=51)
for (i in 1:nrow(af_N50))
{
af_N50[i,]<-sim_genetic_drift2(N=50, t=50)
}
# LARGE N=500 SCENARIO, WITH 20 REPLICATES
af_N500<-matrix(nr=20, nc=51)
for (i in 1:nrow(af_N500))
{
af_N500[i,]<-sim_genetic_drift2(N=500, t=50)
}
# PLOTS ALLELE FREQ TRAJECTORIES
matplot(0:50, t(af_N50), type='l', ylim=c(0, 1),
xlab='generations', ylab='allele frequency', main='N=50', lwd=2)
matplot(0:50, t(af_N500), type='l', ylim=c(0, 1),
xlab='generations', ylab='allele frequency', main='N=500', lwd=2)
par(mfrow=c(1,2))
# PLOTS ALLELE FREQ TRAJECTORIES
matplot(0:50, t(af_N50), type='l', ylim=c(0, 1),
xlab='generations', ylab='allele frequency', main='N=50', lwd=2)
matplot(0:50, t(af_N500), type='l', ylim=c(0, 1),
xlab='generations', ylab='allele frequency', main='N=500', lwd=2)
# MEAN, VARIANCE, AND DISTRIBUTION OF ALLELE FREQ DUE TO DRIFT
# p0=0.5, N=200, t=10
# 10000 INDEPENDENT SIMS (OR LOCI), NOTE THAT WE'RE ONLY INTERESTED IN THE FINAL ALLELE FREQ
final_af<-rep(NA, 10000)
for (i in 1:length(final_af))
{
final_af[i]<-sim_genetic_drift2(N=200, p0=0.5, t=10)[11]
}
# OUR APPROXIMATIONS
mean(final_af)
var(final_af)
hist(final_af, xlab='allele freq', main='Histogram of 10000 final allele freq, given the initial conditions. ')
hist(final_af, xlab='allele freq', main='Histogram of 10000 final allele freq, given the initial conditions. ')
require(doParallel)    # Load the package for parallel computing
cl <- makeCluster(4)   # Create a cluster using 4 CPU threads
registerDoParallel(cl) # Register the cluster for parallel use
# PLOT THE UNCERTAINTY AROUND THESE VARIANCE ESTIMATES (i.e. SAMPLE VARIANCE OF 1000 REPEATED VARIANCE ESTIMTAES)
plot(num_indep_loci, apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='variance (or uncertainty) around the estimate')
# OR PLOT THE RECIPROCAL OF THE UNCERTAINTY?
plot(num_indep_loci, 1/apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='1/variance (or uncertainty) around the estimate')
require(doParallel)    # Load the package for parallel computing
cl <- makeCluster(4)   # Create a cluster using 4 CPU threads
registerDoParallel(cl) # Register the cluster for parallel use
num_indep_loci <- c(50, 100, 200, 500, 1000, 2000)
result <- foreach(i = 1:6, .combine = cbind) %dopar% { #loops over the indices 1 to 6 with each index corresponding to a different value of num_indep_loci. combining results of each iteration of the loop into a single matrix, with each output forming a column. Dopar executes the loop in paralell across cores
temp_result <- rep(NA, 1000) # Store 1000 variance estimates for current setting of numindependentloci
for (j in 1:length(temp_result)) { #loops 1000 times (one for each row in temp_result
temp <- rep(NA, num_indep_loci[i]) # stores a temporary vector for storing final allele frequencies (generation 10) for num independent loci
for (k in 1:num_indep_loci[i]) { #simulate each loci and store final frequency
temp[k] <- sim_genetic_drift2(p0 = 0.5, N = 200, t = 10)[11] #save results into temp
}
temp_result[j] <- var(temp) # Compute variance of final allele frequencies
}
return(temp_result)
}
# PLOT THE UNCERTAINTY AROUND THESE VARIANCE ESTIMATES (i.e. SAMPLE VARIANCE OF 1000 REPEATED VARIANCE ESTIMTAES)
plot(num_indep_loci, apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='variance (or uncertainty) around the estimate')
# OR PLOT THE RECIPROCAL OF THE UNCERTAINTY?
plot(num_indep_loci, 1/apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='1/variance (or uncertainty) around the estimate')
abline(lm(1/apply(result, 2, var)~num_indep_loci), lty=2)
par(mfrow=c(2,1))
# PLOT THE UNCERTAINTY AROUND THESE VARIANCE ESTIMATES (i.e. SAMPLE VARIANCE OF 1000 REPEATED VARIANCE ESTIMTAES)
plot(num_indep_loci, apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='variance (or uncertainty) around the estimate')
# OR PLOT THE RECIPROCAL OF THE UNCERTAINTY?
plot(num_indep_loci, 1/apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='1/variance (or uncertainty) around the estimate')
par(mfrow=c(1,2))
# PLOT THE UNCERTAINTY AROUND THESE VARIANCE ESTIMATES (i.e. SAMPLE VARIANCE OF 1000 REPEATED VARIANCE ESTIMTAES)
plot(num_indep_loci, apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='variance (or uncertainty) around the estimate')
# OR PLOT THE RECIPROCAL OF THE UNCERTAINTY?
plot(num_indep_loci, 1/apply(result, 2, var),
xlab='number of loci used to produce one variance estimate',
ylab='1/variance (or uncertainty) around the estimate')
abline(lm(1/apply(result, 2, var)~num_indep_loci), lty=2)
View(result)
# MODIFY sim_genetic_drift2() TO RUN UNTIL THE LOCUS IS FIXED
sim_genetic_drift3<-function(N=10, p0=0.5)
{
# WE CAN USE UPDATE AND REUSE THE SAME VARIABLE p
p<-p0
# KEEP TRACK OF t
t<-0
# PROPAGATION. WHILE IT REMAINS POLYMORPHIC
while (p>0 & p<1) #while both alleles exist
{
p<-rbinom(1, size=2*N, prob=p)/(2*N) #randomly select an allele out of 0 or 1 for all alleles in population
t<-t+1
}
# RETURN t AND EXIT
return(t)
}
sim_genetic_drift3()
sim_genetic_drift3()
sim_genetic_drift3()
sim_genetic_drift3()
# MEAN PERSISTENCE TIME, AND ITS DISTRIBUTION
# p0=0.02, N=200
# DON'T TRY WITH LARGE N
persistence_time<-rep(NA, 10000)
for (i in 1:length(persistence_time))
{
persistence_time[i]<-sim_genetic_drift3(p0=0.02, N=200)
}
mean(persistence_time)
var(persistence_time)
hist(persistence_time)
mean(persistence_time)
var(persistence_time)
install.packages("ISLR2")
library(tidyverse)  # For data manipulation and visualization
library(sf)         # For spatial data handling
library(tidyverse)
library(terra)
library(raster)
library(sf)
library(sp)
library(dismo)
library(sf)        # core vector GIS package
library(units)     # used for precise unit conversion
library(geodata)   # Download and load functions for core datasets
library(openxlsx)  # Reading data from Excel files
library(rnaturalearth)
library(rnaturalearthdata)
library(ggplot2)
# Download a 1:100m scale world map
world_map <- ne_countries(scale = "small", returnclass = "sf")
data <- read.csv("../data/Data/wheat_data.csv")
setwd("~/Documents/FinalProject/code")
# Download a 1:100m scale world map
world_map <- ne_countries(scale = "small", returnclass = "sf")
data <- read.csv("../data/Data/wheat_data.csv")
# Check the structure
str(data)
summary(data)
# Download a 1:100m scale world map
world_map <- ne_countries(scale = "small", returnclass = "sf")
data <- read.csv("../data/Data/wheat_data.csv")
# Check the structure
str(data)
summary(data)
View(data)
